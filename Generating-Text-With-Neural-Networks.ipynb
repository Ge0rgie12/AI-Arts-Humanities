{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1cdf46",
   "metadata": {},
   "source": [
    "# Generating Text with Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa0b14a",
   "metadata": {},
   "source": [
    "In the following notebook, a generative AI model is being built trained and put to use. Its purpose is to generate text based on a shakespeare dataset so that new text in the style of Shakespeare is being generated.\n",
    "It might also be possible to apply this network to other datasets, using other authors as a basis.\n",
    "This project could for example find use in an interactive digital archive project, helping people get a new few on famous authors. If further developed, it could become an interactive installation that lets people pretend to be talking to for example Shakespeare. \n",
    "\n",
    "The following notebook will be a step by step guide, starting with getting and preparing the data, before building and training the network and lastly trying it out on an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70325e9",
   "metadata": {},
   "source": [
    "# Getting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25967c8d",
   "metadata": {},
   "source": [
    "To start with generating out own text with a neural network, we need a dataset. For this example, a dataset of a Shakespeare text has been chosen.\n",
    "In the following code snippet, the dataset is being imported, so we can work with it. \n",
    "Furthermore, the library tensorflow is being imported, since it is needed to get the dataset file and it is going to be necessary for the following machine learning process\n",
    "The print statement in the second code cell helps exploring the data since it prints out the first few lines of the imported data, helping users to understand what the data looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cebc4dc",
   "metadata": {},
   "source": [
    "### 1. Getting the data for the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b5298b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 12:04:46.152153: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "shakespeare_url = \"https://homl.info/shakespeare\"  # shortcut URL\n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a67cf4",
   "metadata": {},
   "source": [
    "### 2. Visualizing data to help explore it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac39fd44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n"
     ]
    }
   ],
   "source": [
    "print(shakespeare_text[:80]) # not relevant to machine learning but relevant to exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55b2aa",
   "metadata": {},
   "source": [
    "# Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e60a24d",
   "metadata": {},
   "source": [
    "In the following, the data is being prepared. For a neural network to work, the data often needs to have a specific form and adhere to certain rules. This is why it is important to prepare the data before feeding it into the network, to avoid errors in the machine learning process. In this case, for example, the text is first being split by character and transformed into lowercase letters, before being incoded. Encoding is necessary, because neural networks can only work with numbers, not letters, which is why the information has to be transformed into numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dcc276",
   "metadata": {},
   "source": [
    "### 1. Encoding data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96defb6",
   "metadata": {},
   "source": [
    "To encode the data, the characters in the text first need to be split and turned into lower case and then they are being vectorized becfore being incoded into numbers and made into a tensor, so tensorflow can work with it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05fbfd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\",\n",
    "                                                   standardize=\"lower\")\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded = text_vec_layer([shakespeare_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "573e8c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[21  7 10 ... 22 28 12]], shape=(1, 1115394), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(text_vec_layer([shakespeare_text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8519907",
   "metadata": {},
   "source": [
    "### 2. Tokenizing the encoded data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba73049f",
   "metadata": {},
   "source": [
    "In this step the encoded number is being tokenized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcd3ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded -= 2  # drop tokens 0 (pad) and 1 (unknown), which we will not use\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 39\n",
    "dataset_size = len(encoded)  # total number of chars = 1,115,394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e4c79b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 1115394\n"
     ]
    }
   ],
   "source": [
    "print(n_tokens, dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84266cc7",
   "metadata": {},
   "source": [
    "### 3. Preparation of data for network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8571a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b271ad4",
   "metadata": {},
   "source": [
    "### 4. Definition of training, validation and test set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba80acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "tf.random.set_seed(42)\n",
    "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True,\n",
    "                       seed=42)\n",
    "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
    "test_set = to_dataset(encoded[1_060_000:], length=length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d65051",
   "metadata": {},
   "source": [
    "# Building and Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce11e8",
   "metadata": {},
   "source": [
    "In the following, the actual model is being built and trained. To do so, several layers are being added, with each having its own use.\n",
    "Furthermore, the model is being compiled and the loss and accuracy are being printed when training the model, so it can be seen wether the model is performing well or not.\n",
    "In the last line, the number of epochs is being set to 10. An epoch is defined as one pass over the whole dataset. In this case the model would go over the whole dataset 10 times. (Source: https://keras.io/getting_started/faq/#what-do-sample-batch-and-epoch-mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d026aa11",
   "metadata": {},
   "source": [
    "### 1. Building the model and training it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4f19e4cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 12:05:13.341855: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 75997 of 100000\n",
      "2023-11-29 12:05:15.977842: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  31247/Unknown - 1876s 60ms/step - loss: 1.3934 - accuracy: 0.5731INFO:tensorflow:Assets written to: my_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 1911s 61ms/step - loss: 1.3934 - accuracy: 0.5731 - val_loss: 1.5981 - val_accuracy: 0.5345\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 12:37:02.491466: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 93625 of 100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2/31247 [..............................] - ETA: 38:16 - loss: 1.5932 - accuracy: 0.5150   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 12:37:03.192408: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - ETA: 0s - loss: 1.2941 - accuracy: 0.5966INFO:tensorflow:Assets written to: my_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 1902s 61ms/step - loss: 1.2941 - accuracy: 0.5966 - val_loss: 1.5829 - val_accuracy: 0.5408\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 13:08:44.274484: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 70258 of 100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    3/31247 [..............................] - ETA: 34:38 - loss: 1.6381 - accuracy: 0.5042   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 13:08:47.838295: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - ETA: 0s - loss: 1.2764 - accuracy: 0.6005INFO:tensorflow:Assets written to: my_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 1968s 63ms/step - loss: 1.2764 - accuracy: 0.6005 - val_loss: 1.5791 - val_accuracy: 0.5421\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 13:41:32.515837: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 76712 of 100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2/31247 [..............................] - ETA: 38:05 - loss: 1.6262 - accuracy: 0.5025    "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 13:41:37.235630: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - ETA: 0s - loss: 1.2671 - accuracy: 0.6025INFO:tensorflow:Assets written to: my_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: my_shakespeare_model/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31247/31247 [==============================] - 1862s 59ms/step - loss: 1.2671 - accuracy: 0.6025 - val_loss: 1.5757 - val_accuracy: 0.5433\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 14:12:54.579760: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:422] ShuffleDatasetV3:19: Filling up shuffle buffer (this may take a while): 95445 of 100000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    2/31247 [..............................] - ETA: 38:21 - loss: 1.6216 - accuracy: 0.5109   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-29 14:12:55.114123: I tensorflow/core/kernels/data/shuffle_dataset_op.cc:452] Shuffle buffer filled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28077/31247 [=========================>....] - ETA: 3:11 - loss: 1.2623 - accuracy: 0.6037"
     ]
    }
   ],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "]) #building the layers of the network\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"]) \n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10,\n",
    "                    callbacks=[model_ckpt]) #implementing checkpoints to explore loss and accuracy and thereby performance of model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1ede06",
   "metadata": {},
   "source": [
    "### 2. Conclusion about training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a74760",
   "metadata": {},
   "source": [
    "Since it is a very large dataset, the training takes quite a long times. Therefore it is helpful to use a more powerful device (if possible) and to make sure that the device is not running out of power, since that would mean starting the process all over again. Furthermore it is important to take the time it takes to train a model into account, when planning a project that includes machine learning. Additionally, the notebook tends to throw resizing buffer errors when trying to train it again leading to an even longer time effort. \n",
    "![image of error message](screenshots/Error.png)\n",
    "\n",
    "Some research into the error lead to the conclusion that when the error message occurs in the terminal, the training is actually still running, but the jupyter notebook is not showing any output anymore. Therefore, the error is probably more with jupyter notebook instead of the actual code. Several people in a github thread have experienced similar issues, which is also were the conclusion about this problem are stemming from https://github.com/tensorflow/tensorflow/issues/60309\n",
    "Running the training on a laptop for example took 5.5 hours. But about 10 additional hours were needed because of failed training attempts. \n",
    "While the time needed for each epoch can differ, it is pretty similar in this training example, always varying around 1850 and 2000 seconds. In the training excerpt, the epochs and time needed for them are circled in pink, acting as an example of the time needed for training the model. Shown are the times of epoch 1, 2 and 3. \n",
    "![time needed for epochs](screenshots/Training_time.png)\n",
    "\n",
    "In general it can be observed, that the accuracy is getting higher from epoch to epoch, but it is starting at around 0.57, which for me was very surprising, since I had expected for it to start much higher. The accuracy can be seen in the follwing excerpt of the training output:\n",
    "\n",
    "![accuracy of batch 1](screenshots/accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a4ce9",
   "metadata": {},
   "source": [
    "###  3. Defining the shakespeare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0216893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4eacb6",
   "metadata": {},
   "source": [
    "### 4. Peer discussion about time factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceda8b9",
   "metadata": {},
   "source": [
    "The experience witht the time needed to execute all the code and train the model, lead to a peer discussion about the duration of the trianing and its practicality.\n",
    "In the arts and humanities, projects might have limited resources - in computing power, money or time, or all of the above. Therefore it is important to think about how models might be made more time efficient or wether it would be useful for institutions to invest in the needed resources, be it computing power or time. \n",
    "Whenenver using AI in the arts and humanities, it is important to compare the use of resources and the performance of the model. Decreasing the needed time for example, might impact the accuracy of the model negatively. This leads to the conclusion that for each project, a threshhold must be set of a required balance between resources and performance. Project do not have infinite resources, but at the same time, a certain accuracy of the model is needed to produce usable results.\n",
    "It is recommended to think aobut this balance before starting an AI project in the field.\n",
    "\n",
    "In this specific case for example, the accuracy was 0.57, which was lower than expected. It might be increased by investing more time by for example increasing the amount of epochs. But this would also result in a higher time effort, meaning that it would cost more resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8927fee3",
   "metadata": {},
   "source": [
    "# Generating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4276b85",
   "metadata": {},
   "source": [
    "In the following part, the built and trained model is put to use and made to output text. First, some variables need to be defined and then the model is tested, using varying inputs for the temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1f4a38",
   "metadata": {},
   "source": [
    "### 1. Defining variables for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0581742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 1s/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'e'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\n",
    "y_pred = tf.argmax(y_proba)  # choose the most probable character ID\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d31683aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.1129138e-06 8.5561556e-01 2.2224115e-11 2.1224868e-02 3.9113373e-02\n",
      " 3.6693189e-02 4.4674958e-10 4.6583000e-06 9.9596856e-03 2.9044875e-07\n",
      " 4.9154664e-07 1.4862280e-02 5.6615965e-09 1.4430948e-02 3.1604199e-07\n",
      " 8.0804434e-03 8.7489385e-09 1.6814655e-08 2.3241416e-11 1.0369784e-08\n",
      " 2.8587053e-07 4.2391011e-06 4.2892214e-08 4.4083540e-10 1.4786215e-12\n",
      " 2.5570140e-11 1.4088186e-08 3.6063730e-06 1.9268016e-09 4.4550044e-10\n",
      " 1.8006867e-08 3.2127401e-09 3.5330561e-06 8.2488816e-12 1.3907974e-11\n",
      " 6.3178342e-13 2.0576040e-14 1.7938722e-15 2.4389968e-20] tf.Tensor(1, shape=(), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "# help with visualization \n",
    "print(y_proba, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0b80ac03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 8), dtype=int64, numpy=array([[0, 1, 0, 2, 1, 0, 0, 1]])>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]])  # probas = 50%, 40%, and 10%\n",
    "tf.random.set_seed(42)\n",
    "tf.random.categorical(log_probas, num_samples=8)  # draw 8 samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09519ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8b91b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872756c8",
   "metadata": {},
   "source": [
    "### 2. Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9085f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9632ff6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 154ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 60ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "To be or not to be so longer than you shall be so of the strange fro\n"
     ]
    }
   ],
   "source": [
    "# example 1\n",
    "print(extend_text(\"To be or not to be\", temperature=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "77d1a893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 74ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 57ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "To be or not to betweeper than up?\n",
      "what's my honour frame the fire, \n"
     ]
    }
   ],
   "source": [
    "# example 2\n",
    "print(extend_text(\"To be or not to be\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "7c6803df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 55ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 54ms/step\n",
      "1/1 [==============================] - 0s 49ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "1/1 [==============================] - 0s 52ms/step\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "1/1 [==============================] - 0s 36ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "To be or not to be.s?,uf:mya,blqttobcaii.r;d:ngat!r!!ronv:z:z.hyd$s:\n"
     ]
    }
   ],
   "source": [
    "# example 3\n",
    "print(extend_text(\"To be or not to be\", temperature=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4ecab",
   "metadata": {},
   "source": [
    "# Reflection:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e69162",
   "metadata": {},
   "source": [
    "### 1. Evaluating the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c17fcd",
   "metadata": {},
   "source": [
    "The results are interesting, as they vary a lot depending on the value of the temperature. This leads to the conclusion, that \"temperature\" is an important attribute for the generation of new text. While the of 0.01 produces sensible results, a temperature of 100 only results in a string of random keys, as shown in the output of example 3.\n",
    "![output of example 3](screenshots/Random_keys.png)\n",
    "\n",
    "To further investigate this, some more examples with numbers between the above examples can be found below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56953b30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 39ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "To be or not to be so\n",
      "of our love in the king of her business.\n",
      "\n",
      "luci\n"
     ]
    }
   ],
   "source": [
    "# example 4\n",
    "print(extend_text(\"To be or not to be\", temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c33a4555",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 38ms/step\n",
      "To be or not to beju\n",
      "cocio?\n",
      "axs? nujtentiogati swhm, gicfet\n",
      "wa meal,\n"
     ]
    }
   ],
   "source": [
    "# example 6\n",
    "print(extend_text(\"To be or not to be\", temperature=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f83bc475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 41ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 30ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 50ms/step\n",
      "1/1 [==============================] - 0s 42ms/step\n",
      "1/1 [==============================] - 0s 40ms/step\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 45ms/step\n",
      "1/1 [==============================] - 0s 47ms/step\n",
      "1/1 [==============================] - 0s 46ms/step\n",
      "1/1 [==============================] - 0s 44ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 37ms/step\n",
      "1/1 [==============================] - 0s 32ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "To be or not to be so longer than you shall be so of the strange fro\n"
     ]
    }
   ],
   "source": [
    "# example 7\n",
    "print(extend_text(\"To be or not to be\", temperature=0.0000000001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802407d",
   "metadata": {},
   "source": [
    "From these examples, it can be concluded that the lower the temperature, the better is the result. But at some point, the result probably does not change anymore, which can be seen in the last example (example 7). Its temperature is set to 0.0000000001 and it produces the exact same result as example 1, of which the temperature is set to 0.01. Both produce the sentence shown in the screenshot:\n",
    "\n",
    "![output example 1 and 7](screenshots/example_output.png)\n",
    "\n",
    "This means that somewhere is a threshold, after which the result does not change anymore. There is also probably a threshold where the sentence changes from mainly making sense to mainly being non sensical, but this can be very subjective, since some people might find creative ways to make sense of some of the sentences while others do not. Still at some point the output can only be called random and the model does not produce any valuable information anymore. This is all dependent on the temperature. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd2909",
   "metadata": {},
   "source": [
    "### 2. Using own data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc135e1a",
   "metadata": {},
   "source": [
    "I would be interested in using data from other authors, for example Jane Austen, to train the network. It would then be interesting to take the exact same prompt and feed it to the models to then be able to compare the models and resulting texts which each other and (hopefully) have a good example of what different authors would have written about specific prompts.\n",
    "When using other data, the respective dataset would need to be prepared for the network. When using text, the data would need to be vectorized, encoded and tokenized for the machine to be able to work with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da57cc19",
   "metadata": {},
   "source": [
    "### 3. Ethical concerns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fb2f9e",
   "metadata": {},
   "source": [
    "There are two main concerns when it comes to this code or similar projects. On the one hand, there is the issue of copyright. In the case of Shakespeare, his works are by now under a creative commons licence, meaning that everyone can use them, but when implementing own data or using texts by other authors, copyright might become an ethical and also legal concern. Additionally, when it comes to dead people that can not be asked for their consent anymore, it is always an ethical question to what extend their style - like for example Shakespeare's writing style - or their voice and looks can or can not be used nowadays. Therefore it very much depends on the kind of project and wether there for example are in built limiting factors. For example when thinking of Shakespeare and generating texts in his style, a limit could be built in that prevents the machine from putting out bad language when being asked for it. People and their works need to be treated with respect, which is a big ethical concern that always needs to be evaluated when implementing a project such a this one. \n",
    "The other big ethical concern about generative AI and AI in general is the environmental impact. Training and using models is using up a lot of electrical power and thereby a lot of resources. While AI in general can also help with solving environmental issues, its impact on the environment always needs to be taken into consideration when building, training and using a model. Recommended further reading is an article by Payal Dhar \"The carbon impact of artificial intelligence\" which can be found under this link: https://www.nature.com/articles/s42256-020-0219-9#citeas (Source: Dhar, P. The carbon impact of artificial intelligence. Nat Mach Intell 2, 423–425 (2020). https://doi.org/10.1038/s42256-020-0219-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94f81e7",
   "metadata": {},
   "source": [
    "# Final conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd46ba5",
   "metadata": {},
   "source": [
    "In conclusion, the model is successful in generating new texts, but there are ways to improve the performance, on the one hand, the time it takes for the model to train is not ideal for some projects and on the other hand, the accuracy of the model could also be better. Therefore it might make sense to play around with some parts of the training model, for examples the layers of the network or the weights but also the batch size or the amount of epochs, to achieve ever better results. Still the code works and is a good and easy way to for example get into machine learning in the arts and humanities."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
