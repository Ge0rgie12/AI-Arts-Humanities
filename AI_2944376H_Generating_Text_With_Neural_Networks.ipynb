{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df8f69e8",
   "metadata": {},
   "source": [
    "GUID: 2944376H\n",
    "\n",
    "Github respository: https://github.com/Ge0rgie12/AI-Arts-Humanities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1cdf46",
   "metadata": {},
   "source": [
    "# Generating Text with Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa0b14a",
   "metadata": {},
   "source": [
    "In the following notebook, a generative AI model is being built trained and put to use. Its purpose is to generate text, based on a shakespeare dataset so that new text in the style of Shakespeare is being generated.\n",
    "It might also be possible to apply this network to other datasets, using other authors as a basis. The notebook will lead through the steps in the machine learning process and in the end the trained model will be tested by giving it the beginning of a Shakespeare quote and have the model continue the text.\n",
    "\n",
    "This project could for example find use in an interactive digital archive project, helping people get a new view on famous authors. If further developed, it could become an interactive installation that lets people pretend to be talking to for example Shakespeare. \n",
    "\n",
    "The following notebook will be a step by step guide, starting with getting and preparing the data, before building and training the network and lastly trying it out on an example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f632640a",
   "metadata": {},
   "source": [
    "![picture of William Shakespeare](Shakespeare.png)\n",
    "\n",
    "Source: the picture is free to use without attribution by the pixaby content licence \n",
    "\n",
    "\n",
    "Image  of William Shakespeare - will the model be able to imiate his writing style?\n",
    "Let's find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70325e9",
   "metadata": {},
   "source": [
    "# Getting the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25967c8d",
   "metadata": {},
   "source": [
    "To start with generating out own text with a neural network, we need a dataset. For this example, a dataset of Shakespeare texts has been chosen. The format of the data is a txt file, meaning it is in pure text format.\n",
    "In the following code snippet, the dataset is being imported, so we can work with it. \n",
    "Furthermore, the library tensorflow is being imported, since it is needed to get the dataset file and it is going to be necessary for the following machine learning process. Tensorflow is a very common library used for machine learning and building neural networks. \n",
    "The print statement in the second code cell helps exploring the data since it prints out the first few lines of the imported data, helping users to understand what the data looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cebc4dc",
   "metadata": {},
   "source": [
    "### 1. Getting the data for the network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5298b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "\n",
    "shakespeare_url = \"https://homl.info/shakespeare\"  # shortcut URL\n",
    "filepath = tf.keras.utils.get_file(\"shakespeare.txt\", shakespeare_url)\n",
    "with open(filepath) as f:\n",
    "    shakespeare_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a67cf4",
   "metadata": {},
   "source": [
    "### 2. Visualizing data to help explore it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39fd44",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(shakespeare_text[:80]) # not relevant to machine learning but relevant to exploring the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3621adc5",
   "metadata": {},
   "source": [
    "Visualizing data does not only help with exploring and better understanding it, but also with checking wether it was imported correctly or wether everything is correct. Soemtimes the first errors can be found through checking this. Furthermore, if working with tabular data for example, it is useful to see wether there are any missing values.\n",
    "In this case the data is in text form. As shown in the example above, which are the first 80 characters in the dataset, as indicated in the interval [:80], and they are simply the first lines in a Shakespeare play. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55b2aa",
   "metadata": {},
   "source": [
    "# Preparing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e60a24d",
   "metadata": {},
   "source": [
    "In the following, the data is being prepared. For a neural network to work, the data often needs to have a specific form and adhere to certain rules. This is why it is important to prepare the data before feeding it into the network, to avoid errors in the machine learning process. In this case, for example, the text is first being split by character and transformed into lowercase letters, before being encoded. Encoding is necessary, because neural networks can only work with numbers, not letters, which is why the information has to be transformed into numbers. \n",
    "\n",
    "For the validation of results, the print statements are useful, als they show the form of the transformed data and therefore show wether the results are as expected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10dcc276",
   "metadata": {},
   "source": [
    "### 1. Encoding data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96defb6",
   "metadata": {},
   "source": [
    "To encode the data, the characters in the text first need to be split and turned into lower case and then they are being vectorized becfore being incoded into numbers and made into a tensor, so tensorflow can work with it.\n",
    "For example the \"split\" by \"character\" is a set way of splitting the input data. It means that the input text is being split at each unicode character. The explanation for this can be found in the tensorflow documentation https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization. \n",
    "\n",
    "The tf.keras.layers.TextVectorization layer is a keras preprocessing layer that transform natural language input into numerical data. More information on how it works and how to use the layer can be found in the tensorflow documentation https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fbfd0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = tf.keras.layers.TextVectorization(split=\"character\",\n",
    "                                                   standardize=\"lower\")\n",
    "text_vec_layer.adapt([shakespeare_text])\n",
    "encoded = text_vec_layer([shakespeare_text])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "573e8c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_vec_layer([shakespeare_text]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8519907",
   "metadata": {},
   "source": [
    "### 2. Tokenizing the encoded data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba73049f",
   "metadata": {},
   "source": [
    "In this step the encoded number is being tokenized "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd3ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded -= 2  # drop tokens 0 (pad) and 1 (unknown), which we will not use\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 39\n",
    "dataset_size = len(encoded)  # total number of chars = 1,115,394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4c79b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_tokens, dataset_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84266cc7",
   "metadata": {},
   "source": [
    "### 3. Preparation of data for network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68b39374",
   "metadata": {},
   "source": [
    "In this section, the dataset is being prepared for further use. Furthermore, the batch size is being set to 32. When having a look at stackoverflow, 32 seems to be considered a standard batch size, especially when starting out with training a model. https://stackoverflow.com/questions/35050753/how-big-should-batch-size-and-number-of-epochs-be-when-fitting-a-model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8571a70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_dataset(sequence, length, shuffle=False, seed=None, batch_size=32):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence)\n",
    "    ds = ds.window(length + 1, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda window_ds: window_ds.batch(length + 1))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    return ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b271ad4",
   "metadata": {},
   "source": [
    "### 4. Definition of training, validation and test set "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95fe9f7",
   "metadata": {},
   "source": [
    "In this part the dataset is being split into a training, validation and test set. This is done by using the indices of the encoded data. The dataset is simply split at two points to make the three sets. \n",
    "These three different sets are needed for different steps in the machine learning process. The training set is used to acutally train the network, after which it will be able to produce an output. The test set is then used to test the accuracy and the use of the network. After testing, the network might have to be adjusted, resulting in a new training round with the training set. This cycle can be repeated a couple of times. The validation set is then used for the final step. After repeating the trianing and testing a few times, the network might have learned the data in the test set, meaning it will be able to predict the right answer simply because it knows the information from experience and learning and not through training. Therefore, after the network is considered to be finished, with the accuracy and performance being good, the validation set is used to test the network again but on unknown data. The validation is usually only used once because its whole purpose is that it contains previously unknown data to be able to test wether the network is actually performing well or wether it simply has learned all the test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba80acc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "length = 100\n",
    "tf.random.set_seed(42)\n",
    "train_set = to_dataset(encoded[:1_000_000], length=length, shuffle=True,\n",
    "                       seed=42)\n",
    "valid_set = to_dataset(encoded[1_000_000:1_060_000], length=length)\n",
    "test_set = to_dataset(encoded[1_060_000:], length=length)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d65051",
   "metadata": {},
   "source": [
    "# Building and Training the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce11e8",
   "metadata": {},
   "source": [
    "In the following, the actual model is being built and trained. To do so, several layers are being added, with each having its own use. While understanding each layer is not necessary to understand machine learning in the context of the arts and humanities, further information on them is provided in the tensorflow documentation.\n",
    "Furthermore, the model is being compiled and the loss and accuracy are being printed when training the model, so it can be seen wether the model is performing well or not.\n",
    "In the last line, the number of epochs is being set to 10. An epoch is defined as one pass over the whole dataset. In this case the model would go over the whole dataset 10 times. (Source: https://keras.io/getting_started/faq/#what-do-sample-batch-and-epoch-mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d026aa11",
   "metadata": {},
   "source": [
    "### 1. Building the model and training it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f19e4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=n_tokens, output_dim=16),\n",
    "    tf.keras.layers.GRU(128, return_sequences=True),\n",
    "    tf.keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "]) #building the layers of the network\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\",\n",
    "              metrics=[\"accuracy\"]) \n",
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\n",
    "history = model.fit(train_set, validation_data=valid_set, epochs=10,\n",
    "                    callbacks=[model_ckpt]) #implementing checkpoints to explore loss and accuracy and thereby performance of model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1ede06",
   "metadata": {},
   "source": [
    "### 2. Conclusion about training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86a74760",
   "metadata": {},
   "source": [
    "Since it is a very large dataset, the training takes quite a long times. Therefore it is helpful to use a more powerful device (if possible) and to make sure that the device is not running out of power, since that would mean starting the process all over again. Furthermore it is important to take the time it takes to train a model into account, when planning a project that includes machine learning. Additionally, the notebook tends to throw resizing buffer errors when trying to train it again leading to an even longer time effort. \n",
    "![image of error message](screenshots/Error.png)\n",
    "\n",
    "Some research into the error lead to the conclusion that when the error message occurs in the terminal, the training is actually still running, but the jupyter notebook is not showing any output anymore. Therefore, the error is probably more with jupyter notebook instead of the actual code. Several people in a github thread have experienced similar issues, which is also were the conclusion about this problem are stemming from https://github.com/tensorflow/tensorflow/issues/60309. In the terminal, it can still be observed when the next epoch is starting, the jupyter notebook interface simply is not showing the output anymore. After the training has finished, the changes have to be saved and then the notebook needs to be shutdown and reopened. After that, the trained model can be used in the further process and the following cells will be showing output again, which was not possible before restarting the notebook.\n",
    "Running the training on a laptop for example took 5.5 hours. But about 20 additional hours were needed because of failed training attempts. In the last attempts, by coincidence all of the training data was actually shown in the output of the notebook. Through this it was possible to observe that the accuracy went up to 0.6. To be able to oberserve the development on the accuracy and loss was the reason to attempt the training several times, inspite of the time needed to do so. The reason for this is that the accuracy is very telling for how well a model will perform. \n",
    "While the time needed for each epoch can differ, it is pretty similar in this training example, always varying around 1850 and 2000 seconds. In the training excerpt, the epochs and time needed for them are circled in pink, acting as an example of the time needed for training the model. Shown are the times of epoch 1, 2 and 3. \n",
    "![time needed for epochs](screenshots/Training_time.png)\n",
    "\n",
    "In general it can be observed, that the accuracy is getting higher from epoch to epoch, but it is starting at around 0.57, which for me was very surprising, since I had expected for it to start much higher, as in the machine learning by example from start to end it had already startd at around 0.94. This leads to the conclusion, that the model could maybe use some work to make it more accurate, but at the same time the accuracy is incresing during the training process, meaning the model is already a good start. How well it actually works will be showed later in the notebook, when testing it on some example cases. The accuracy can be seen in the follwing excerpt of the training output:\n",
    "\n",
    "![accuracy of batch 1](screenshots/accuracy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6a4ce9",
   "metadata": {},
   "source": [
    "###  3. Defining the shakespeare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0216893d",
   "metadata": {},
   "outputs": [],
   "source": [
    "shakespeare_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n",
    "    model\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4eacb6",
   "metadata": {},
   "source": [
    "### 4. Peer discussion about time factor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bceda8b9",
   "metadata": {},
   "source": [
    "The experience witht the time needed to execute all the code and train the model, lead to a peer discussion about the duration of the trianing and its practicality.\n",
    "In the arts and humanities, projects might have limited resources - in computing power, money or time, or all of the above. Therefore it is important to think about how models might be made more time efficient or wether it would be useful for institutions to invest in the needed resources, be it computing power or time. \n",
    "Whenenver using AI in the arts and humanities, it is important to compare the use of resources and the performance of the model. Decreasing the needed time for example, might impact the accuracy of the model negatively. This leads to the conclusion that for each project, a threshhold must be set of a required balance between resources and performance. Project do not have infinite resources, but at the same time, a certain accuracy of the model is needed to produce usable results. Another option would be to reduce the size of the data fed to the model, since each epoch means going over each data point in the training set, so reducing the size of the training set means each epoch takes less time because there is less data to go over. But this would mean that the model would have less data to train on, which could also result in a lower accuracy and worse performance of the model. \n",
    "It is recommended to think aobut this balance before starting an AI project in the field.\n",
    "\n",
    "In this specific case for example, the accuracy was 0.57, which was lower than expected. The highest accuracy stated was 0.6 after the tenth epoch. It might be increased by investing more time by for example increasing the amount of epochs. But this would also result in a higher time effort, meaning that it would cost more resources.\n",
    "\n",
    "Another factor could be the batch size. In 2016, user Lucas Ramadan listed some general rules about batch size on a thread on stackoverflow. A larger batch size might therefore help to make the training process faster, but not in all cases. https://stackoverflow.com/questions/35050753/how-big-should-batch-size-and-number-of-epochs-be-when-fitting-a-model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8927fee3",
   "metadata": {},
   "source": [
    "# Generating Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4276b85",
   "metadata": {},
   "source": [
    "In the following part, the built and trained model is put to use and made to output text. First, some variables need to be defined and then the model is tested, using varying inputs for the temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af1f4a38",
   "metadata": {},
   "source": [
    "### 1. Defining variables for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0581742",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_proba = shakespeare_model.predict([\"To be or not to b\"])[0, -1]\n",
    "y_pred = tf.argmax(y_proba)  # choose the most probable character ID\n",
    "text_vec_layer.get_vocabulary()[y_pred + 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31683aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# help with visualization \n",
    "print(y_proba, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb67faa",
   "metadata": {},
   "source": [
    "As in the steps before, print statements, as the one before, can help with visualization and also verification of code that has been executed before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d16b44",
   "metadata": {},
   "source": [
    "In the following code cell, some mathematical operations are being used to define probabilites and to draw 8 samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b80ac03",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_probas = tf.math.log([[0.5, 0.4, 0.1]])  # probas = 50%, 40%, and 10%\n",
    "tf.random.set_seed(42)\n",
    "tf.random.categorical(log_probas, num_samples=8)  # draw 8 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87d3648",
   "metadata": {},
   "source": [
    "In the following part of this step the function next_char is being defined. It takes text and temperature as arguments and returns text_vec_layer.get_vocabulary()[char_id + 2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09519ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_char(text, temperature=1):\n",
    "    y_proba = shakespeare_model.predict([text])[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a411bfa3",
   "metadata": {},
   "source": [
    "In the last step of this part, the function extend_text is being defined. It takes the arguments text, n_chars=50, temperature=1 and returns a text when being called. Text is a variable that is also being defined in this part of the code.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b91b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872756c8",
   "metadata": {},
   "source": [
    "### 2. Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9085f1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9632ff6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 1\n",
    "print(extend_text(\"To be or not to be\", temperature=0.01))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d1a893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 2\n",
    "print(extend_text(\"To be or not to be\", temperature=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6803df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 3\n",
    "print(extend_text(\"To be or not to be\", temperature=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f4ecab",
   "metadata": {},
   "source": [
    "# Reflection:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e69162",
   "metadata": {},
   "source": [
    "### 1. Evaluating the result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38c17fcd",
   "metadata": {},
   "source": [
    "The results are interesting, as they vary a lot depending on the value of the temperature. This leads to the conclusion, that \"temperature\" is an important argument for the generation of new text. While the of 0.01 produces sensible results, a temperature of 100 only results in a string of random keys, as shown in the output of example 3.\n",
    "![output of example 3](screenshots/Random_keys.png)\n",
    "\n",
    "To further investigate this, some more examples with numbers between the above examples can be found below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56953b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 4\n",
    "print(extend_text(\"To be or not to be\", temperature=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33a4555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 6\n",
    "print(extend_text(\"To be or not to be\", temperature=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f83bc475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example 7\n",
    "print(extend_text(\"To be or not to be\", temperature=0.0000000001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c802407d",
   "metadata": {},
   "source": [
    "From these examples, it can be concluded that the lower the temperature, the better is the result. But at some point, the result probably does not change anymore, which can be seen in the last example (example 7). Its temperature is set to 0.0000000001 and it produces the exact same result as example 1, of which the temperature is set to 0.01. Both produce the sentence shown in the screenshot:\n",
    "\n",
    "![output example 1 and 7](screenshots/example_output.png)\n",
    "\n",
    "This means that somewhere is a threshold, after which the result does not change anymore. There is also probably a threshold where the sentence changes from mainly making sense to mainly being non sensical, but this can be very subjective, since some people might find creative ways to make sense of some of the sentences while others do not. Still at some point the output can only be called random and the model does not produce any valuable information anymore. This is all dependent on the temperature. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fd2909",
   "metadata": {},
   "source": [
    "### 2. Using own data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc135e1a",
   "metadata": {},
   "source": [
    "I would be interested in using data from other authors, for example Jane Austen, to train the network. It would then be interesting to take the exact same prompt and feed it to the models to then be able to compare the models and resulting texts which each other and (hopefully) have a good example of what different authors would have written about specific prompts. In the larger context of possible projects this network could me useful for, an interactive exhibition with several different authors could be a great experience. Therefore this project would be useful for cultural heritage institutions, especially literary ones, like the british library for example. \n",
    "When using other data, the respective dataset would need to be prepared for the network. When using text, the data would need to be vectorized, encoded and tokenized for the machine to be able to work with it. Since data is a very unique topic, this part will probably be adjusted the most. \n",
    "\n",
    "Furthermore, this project could be extended beyond the area of authors and literature. To do so, though, more parts of the training might have to be adjusted. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da57cc19",
   "metadata": {},
   "source": [
    "### 3. Ethical concerns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29fb2f9e",
   "metadata": {},
   "source": [
    "There are two main concerns when it comes to this code or similar projects. On the one hand, there is the issue of copyright. In the case of Shakespeare, his works are by now under a creative commons licence, meaning that everyone can use them, but when implementing own data or using texts by other authors, copyright might become an ethical and also legal concern. Additionally, when it comes to dead people that can not be asked for their consent anymore, it is always an ethical question to what extend their style - like for example Shakespeare's writing style - or their voice and looks can or can not be used nowadays. Therefore it very much depends on the kind of project and wether there for example are in built limiting factors. For example when thinking of Shakespeare and generating texts in his style, a limit could be built in that prevents the machine from putting out bad language when being asked for it. People and their works need to be treated with respect, which is a big ethical concern that always needs to be evaluated when implementing a project such a this one. \n",
    "The other big ethical concern about generative AI and AI in general is the environmental impact. Training and using models is using up a lot of electrical power and thereby a lot of resources. While AI in general can also help with solving environmental issues, its impact on the environment always needs to be taken into consideration when building, training and using a model. Recommended further reading is an article by Payal Dhar \"The carbon impact of artificial intelligence\" which can be found under this link: https://www.nature.com/articles/s42256-020-0219-9#citeas (Source: Dhar, P. The carbon impact of artificial intelligence. Nat Mach Intell 2, 423–425 (2020). https://doi.org/10.1038/s42256-020-0219-9)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94f81e7",
   "metadata": {},
   "source": [
    "# Final conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd46ba5",
   "metadata": {},
   "source": [
    "In conclusion, the model is successful in generating new texts, but there are ways to improve the performance, on the one hand, the time it takes for the model to train is not ideal for some projects and on the other hand, the accuracy of the model could also be better. Therefore it might make sense to play around with some parts of the training model, for examples the layers of the network or the weights but also the batch size or the amount of epochs, to achieve ever better results. Still the code works and is a good and easy way to for example get into machine learning in the arts and humanities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea0a7db",
   "metadata": {},
   "source": [
    "This concludes the Exercise with generating text with a neural network!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e216a225",
   "metadata": {},
   "source": [
    "![image of blackboard with well done written on it](well_done.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04d3972",
   "metadata": {},
   "source": [
    "Source: the picture is free to use without attribution by the pixaby content licence "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afda2fff",
   "metadata": {},
   "source": [
    "Recommended resources for further reading:\n",
    "\n",
    "- http://stackoverflow.com\n",
    "- The tensorflow documentation https://www.tensorflow.org\n",
    "- And a personal favorite, the website https://www.geeksforgeeks.org\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
