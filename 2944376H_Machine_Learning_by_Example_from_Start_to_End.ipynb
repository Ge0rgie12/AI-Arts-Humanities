{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c94e506",
   "metadata": {},
   "source": [
    "GUID: 2944376H\n",
    "\n",
    "Github respository: https://github.com/Ge0rgie12/AI-Arts-Humanities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c259f2",
   "metadata": {},
   "source": [
    "# Machine Learning by Example: from Start to End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6539a1d",
   "metadata": {},
   "source": [
    "# Task 1: README!\n",
    "\n",
    "#### The tasks in this notebook should be tackled in discussion with your peer group! \n",
    "\n",
    "**By asking and answering each other questions, you learn much more than doing independent work.** The article [“Embracing Digitalization: Student Learning and New Technologies” (Crittenden, Biel & Lovely 2018)](https://journals.sagepub.com/doi/10.1177/0273475318820895) shows how we learn and retain more information when we explain and discuss it with others.\n",
    "\n",
    "Before you tackle a machine learning project you should make sure you keep the bigger picture in your head - this is your human input, skill, and imagination -  no AI can do this for you at the moment. The following sketches the typical steps involved in a machine learning project:\n",
    "\n",
    "- Step 1: Frame Your Problem\n",
    "    - What is the task? - Who will use it in what environment? What are the risks and impact?\n",
    "    - How will you measure performance of your model? Measures sufficient to assess potential risks and impacts?\n",
    "    - What are the assumptions? Document and review assumptions for bias. Question everything.\n",
    "- Step 2: Get Your Data\n",
    "    - Download your data - How will your get your data from where? Permissions and licenses? Suitable and reliable?\n",
    "    - Take a quick look at the data structure - how big is it? what fields/attributes are there and how many?\n",
    "    - Set aside test data - random split or stratified split? \n",
    "- Step 3: Prepare Your Data\n",
    "    - Handling Text/Categorical Data\n",
    "    - Scaling and Transformation\n",
    "    - Separate the labels from the rest of the attributes\n",
    "- Step 4: Select and Train Your Model\n",
    "    - train and evaluate on the training set\n",
    "    - cross-validation\n",
    "- Step 6: Test on Completely New Data\n",
    "- Step 7: Publish Your Results! Party! &#x1F389; &#x1F389; &#x1F389;\n",
    "\n",
    "In this notebook, we will go through some of the key the steps. Some tasks will involve critical reflection, and others will be about coding. \n",
    "\n",
    "Remember that, **if you are taking more than 30 minutes to do one task without any progress**, you should probably take a break. \n",
    "- Note down what you did and what errors you got in a markdown cell. This will help you understand the recurring errors, you will understand where you left off when you come back to it later, and also help you when you discuss the problem with your peers and with the lab tutors.  \n",
    "\n",
    "The code in this notebook is modified from that which was made available by Aurélien Géron and his fabulous book [\"Hands-On Machine Learning with Scitkit-Learn, Keras & Tensorflow\"](https://eleanor.lib.gla.ac.uk/record=b4094676)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6186da91",
   "metadata": {},
   "source": [
    "## Task 1-1: Before You Start: Prepare Your Computing Environment \n",
    "\n",
    "### Step 1: First, open Glasgow Anywhere Remote Desktop. Use the Student Desktop. \n",
    "You can use your own machine but it can take more time to set up just so for your course work. The remote desktop, in contrast, has almost every package.\n",
    "\n",
    "### Step 2: Go through the standard approach to opening a notebook. \n",
    "- Open a browser (recommend Chrome incognito mode). \n",
    "- Navigate to the course moodle.\n",
    "- Download the notebook linked at `Machine Learning by Example from Start to End` from the course [Resources section](https://moodle.gla.ac.uk/course/view.php?id=39566#section-3). Save it in your course project folder on `One Drive - University of Glasgow`.\n",
    "- Start Anaconda Navigator. \n",
    "- Launch Jupyter Notebook. \n",
    "- Navigate to your course project folder. Open up the notebook you downloaded.\n",
    "\n",
    "Do not opening a Jupyter Notebook directly – always go through Anaconda Navigator. This allows you to clearly see which version notebook you are opening. The correct version leads to the availability of necessary Python libraries.\n",
    "\n",
    "### Step 3: Prepare to upload material to your GitHub repository. \n",
    "Open a web browser (recommend Google Chrome in incognito mode). Navigate to GitHub and log in. Navigate to your repository for the course AI for the Arts and Humanities (A).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6025a3c8",
   "metadata": {},
   "source": [
    "## Task 1-2: Checking Your Set Up\n",
    "\n",
    "It is important not only to check that you have the correct set of software and packages, but also that the version is the right one. If versions are not compatible with your code then it will throw up errors or unexpected results. This is why you need to make these requirements known to people you share your code with (by, for example, by accompanying your code with a requirements.txt file, as included in your previous lab exrcises).\n",
    "\n",
    "### Python\n",
    "\n",
    "Check that your Python has version greater than 3.7 using the following code. This is what the code in the noteboook requires."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb2a627b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys # importing the package sys which lets you talk to your computer system.\n",
    "\n",
    "assert sys.version_info >= (3, 7) #versions are expressed a pair of numbers (3, 7) which is equivalent to 3.7. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4515d4df",
   "metadata": {},
   "source": [
    "Note: Tried with numbers 9, 10, 11 and 15. Up to 11 everythings was fine, 15 threw an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eb61a85",
   "metadata": {},
   "source": [
    "The `assert` statement throws up an error when the statement following it is not true. If it is true, nothing will be shown. Experiment by replacing the numbers in the round brackets to be much bigger. **A Pair of numbers** like `(3, 7)` in round brackets is a data structure known as a **tuple** in programming lingo. \n",
    "\n",
    "### Scikit-Learn\n",
    "\n",
    "Check that your Scikit-Learn package version is greater than 1.0.1. \n",
    "\n",
    "In this case you will need to import `version` which is part of the `packaging` Python library. This allows you to extract/parse version numbers for Python packages/libraries like `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5882a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from packaging import version #import the package \"version\"\n",
    "import sklearn # import scikit-learn\n",
    "\n",
    "assert version.parse(sklearn.__version__) >= version.parse(\"1.0.1\") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c95bc8",
   "metadata": {},
   "source": [
    "Note: Needed packages are imported and since there is no error, the package version of Scikit-Learn is greater that 1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100761e5",
   "metadata": {},
   "source": [
    "### Fonts Used in Figures\n",
    "\n",
    "The following code sets some font sizes to be used with `matplotlib.pyplot` (recall we used matplotlib in previous exercises to display visual information or data). You can set different sizes if you like, but too big and it won't look nice, too little and it will illegible. The code is intended to **prettify** your figures to look nicer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b10286e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rc('font', size=14) #general font size\n",
    "plt.rc('axes', labelsize=14, titlesize=14) #font size for the titles of x and y axes\n",
    "plt.rc('legend', fontsize=14) # font size for legends\n",
    "plt.rc('xtick', labelsize=10) # the font size of labels for intervals marked on the x axis\n",
    "plt.rc('ytick', labelsize=10) # the font size of labels for intervals marked on the y axis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a039fb99",
   "metadata": {},
   "source": [
    "### Creating the Folder for Images \n",
    "\n",
    "The code below creates the directory `images/classification` (if it doesn't already exist) and defines the `save_fig()` function which is used to save the figures you create in matplotlib in high resolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75f5275f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "IMAGES_PATH = Path() / \"images\" / \"classification\"\n",
    "IMAGES_PATH.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = IMAGES_PATH / f\"{fig_id}.{fig_extension}\"\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dafd09c",
   "metadata": {},
   "source": [
    "## Task 1-3: Review Machine Learning\n",
    "\n",
    "### Step 1: Create a markdown cell to demonstrate your own reflection\n",
    "- Follow the instructions in Part 1, Task 1, to open your Python notebook. And Create a Markdown cell.\n",
    "- In your markdown cell embed an image or link to a diagram illustrating the workflow from data to algorithm to model and data to model to predicted output. \n",
    "    - To embed images in your markdown cell, you can use the syntax `![alt text](image.jpg)` where you replace alt text with a description of your image (keep the square brackets!) and replace image.jpg with the file path and name of your image. \n",
    "    - To include a URL, use the syntax `[title](https://www.example.com)` where you replace title  with your own description, and https://www.example.com  with your own URL. Keep all brackets intact. \n",
    "    - For your reference, you can refer to the [markdown cheat sheet](https://www.markdownguide.org/cheat-sheet/)  - note that HTML codes are also understood by your notebook.\n",
    "- Explain in your markdown cell how the examples in Lectures 3 & 4 align with the workflow. For example, what is the data, what was the learning algorithm, what is the model and what did the model output in response to new data?\n",
    "- Reflect on the range of ways to explain the workflow and the examples to a wider audience, for example, a museum curator?\n",
    "\n",
    "### Step 2: Discuss and report your reflection with your group\n",
    "- Get together with your peer group. Take turns to discuss your reflection above. If you have any difficulties, discuss these also.\n",
    "- Note down the results of your discussion in your notebook. In particular, note down anything that help you or others learn the topic. What approach could take in your notebook to engage the wider audience with your machine learning code.\n",
    "I've created a cell for you to use already below - double click on the area to start editing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fdf563",
   "metadata": {},
   "source": [
    "***Markdown Cell***\n",
    "\n",
    "### Step 1: \n",
    "\n",
    "1. Here is an image of a workflow from data to model to predicted output: ![image of workflow of data to model to predicted output](images/diagram.jpeg)\n",
    "\n",
    "#### Source: \n",
    "Yu Xuan Lee: \"Converting a Deep Learning Model with Multiple Outputs from PyTorch to TensorFlow\", 18 August 2019, Published in Towards Data Science https://towardsdatascience.com/converting-a-deep-learning-model-with-multiple-outputs-from-pytorch-to-tensorflow-a2d27a8e44f4, picture reference: https://i.stack.imgur.com/4YBht.jpg\n",
    "\n",
    "The article also provides a good additional overview of the machine learning workflow and is worth a read.\n",
    "\n",
    "\n",
    "2. The leukemia example in lecture 3 can be explained with the workflow in the following way: the names of the people that are being looked at are the data. The learning algorithm is the part where the machine classifies everyone called Luke as having leukemia and everone else with a different name as not having leukemia. The model then put out how many people had leukeia and how many did not, using a decision tree. This was then tested with the actual data. Overall, and also concerning new data the model put out information on wether a person had leukemia or not based on their name. The decision was made on the basis of wether a person was callded Luke. This information can be sorted into the groups right positive, false positive, right negative and false negative. \n",
    "3. The spam filtering example from lecture 4: The data in this case are emails, that can either be sorted into spam or not spam. To train the model, the data is being labeled as spam or not spam, so connections can be made. The model itself is a neural network with several hidden layers. Between the layers, connections are being made. In response to new data, the model outputs wether an unknown email is spam or not. If there is a mistake, the connections between the layers are being changed and thereby the model is being trained further to label new data more accurately in the future. \n",
    "4. To explain the workflow to a wider audience, for example a museum curator, diagrams can be a useful tool to make it easier to understand. Furthermore, examples, especially from the specific field the audience is from, are very helpful. For a museum curator, for example, the workflow can be explained using a network that classifies artworks or other pieces. Additionally, the examples can be explained by breaking them down into smaller steps and concentrating on the different parts of the workflow step by step. Besides that, a practical connection to the actual work of the audience and the real life use of the examples and models is helpful. In conclusion, there are many different ways to explain the examples and the workflow, but both examples and practical connection are valuable for it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efd8083",
   "metadata": {},
   "source": [
    "### Step 2:\n",
    "1. reflections and procedures are very similar in the peer group, aside from the last question, which was approached in very different ways. The biggest problem was finding good pictures for the first parts of step 1, since many diagrams on the internet are either over-complicated or too specific to fit. \n",
    "2. To help others learn the topic, real world examples are important, as well as breaking down the process into smaller steps, to make them easier to understand. In this notebook, there are already a lot of information and comments around the code, which can help to engage a wider audience with the notebook. An additional step to achieve this would be to reflect on the various steps and maybe add information on erros that occured while executing the code and how to solve them, to help a wider audience get through the notebook with less problems and thereby help them stay interested in the topic and make the whole topic more approachable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "027e97d4",
   "metadata": {},
   "source": [
    "## Task 1-4: Framing the Problem\n",
    "\n",
    "In this notebook, we will be working with two datasets:\n",
    "\n",
    "1)\tTabular data consisting of information about houses in districts within the US state of California, and, \n",
    "2)\tImage pixel data, each image representing a digit handwritten by high school students and employees of the US Census Bureau. \n",
    "\n",
    "The first of these datasets will be used to **predict median housing prices for a given district**. The results of the prediction will be combined with other data to determine whether it is worth investing in a given district. \n",
    "\n",
    "The second of these datasets will be used to **classify hand written digits**. It was originally developed as a way of sorting out the handwritten US zip codes (similar to UK postcodes) at the post office. \n",
    "\n",
    "\n",
    "### Step 1: Understand how framing the problem affects data selection\n",
    "\n",
    "- The academic article [“Rethinking the field of automatic prediction of court decisions”](https://link.springer.com/article/10.1007/s10506-021-09306-3) by Medvedeva, Wieling & Vols (2023), to appreciate how, depending on the objectives, the characteristics of data and algorithm might differ. \n",
    "- Read the BBC article [“AI facial recognition: Campaigners and MPs call for ban”](https://www.bbc.co.uk/news/technology-67022005) to understand that the same data, depending on its use, can raise concern about AI. We will be discussing prediction court decisions further in Week 7.\n",
    "- In view of the above, write down your reflection on the importance of framing your problem precisely in a notebook markdown cell - not only to define the task properly, but to understand how your machine learning model will be used down the road. \n",
    "\n",
    "### Step 2: How to select your algorithm\n",
    "\n",
    "In Lecture 2, we discussed how machine learning can be divided into three types: Supervised, Unsupervised, and Reinforcement. Large part of this course is focused on supervised learning – in particular, in this notebook, we will explore this using examples of regression and classification.\n",
    "\n",
    "**To refresh your memory, read this short article from Codecademy** – [“Regression vs Classification”](https://www.codecademy.com/article/regression-vs-classification).\n",
    "\n",
    "- Discuss with your peer group whether regression or classification would fit better for predicting median housing prices.\n",
    "- Discuss with your peer group whether regression or classification would fit better for handwritten digit recognition.\n",
    "- Write down the results of the discussion. In particular, report on what you concluded after the discussion and why.\n",
    "\n",
    "### Step 3: Before Data Collection\n",
    "\n",
    "Once your problem is defined (e.g. predicting the median housing price of a district), you will need to collect a new data set appropriate for your task, and/or identify existing data sets that can be used for training your model. \n",
    "\n",
    "- Discuss with your peer group what kind of information about housing in a district you think would help predict the median housing price in the district.\n",
    "- Discuss how these decisions might depend on geographical and/or cultural differences and how the information you collect would already bias the data. \n",
    "\n",
    "**Note these down the results of the discussions in Steps 2 & 3 in a markdown cell below.** I have already created a markdown cell for your use - just double click the area to begin editing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e3715aa",
   "metadata": {},
   "source": [
    "***Markdown cell for discussions***\n",
    "\n",
    "### Step 1:\n",
    "#### Framing a problem\n",
    "It is important to frame the problem precicesly for various reasons. First, it is needed to be able to choose a fitting dataset. Second, there are different types of machine learning algorithms and models, so defining the problem is necessary to be able to choose the right model or network. Additionally, a clear definition of the problem is important for being aware of possible bias integrated in the machine learning process. How a problem is being framed can have a huge impact on wether a model will be biased and in which direction that bias will lean. Therefore, it is important to frame a problem precicesly so a model can be used and understood correctly in the future and the code might be transferred to a similar problem and the model used for other things in the future.\n",
    "\n",
    "### Step 2:\n",
    "#### Which algorithm to choose\n",
    "Predicting median housing prices: regression because the output is continous and is not being sorted into labels, since the output can be any number and is not being chosen from a specified set of labels. (comp. Codeacademy  “Regression vs Classification”)\n",
    "Handwritten digit recognition: classification because the pictures are being sorted into different groups --> classes and therefore a classification algorithm is more fitting. \n",
    "\n",
    "### Step 3:\n",
    "#### Choosing a dataset\n",
    "The price of property or rent prices are important, depending on wether the median price for buying or renting a property is being looked at. Furhtermore, the reputation of districs, e.g. wether it is commonly considered to be a more expensive or cheaper district to live in.\n",
    "While the data would very much differ depending on the geographical and/or cultural differences, the decisions on which data to collect do not vary that much. I would still try to collect data on the prices of housing and the reputation of a district. Especially the second point could lead to bias, though, depending on who is being asked about the reputation of a district - for example wether the group of questioned people is rather divers or rather homogenous. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ae4ec",
   "metadata": {},
   "source": [
    "# Working with Data\n",
    "\n",
    "Overviews of machine learning and AI often make it seem as though the largest part of machine learning is in training the algorithm. This is misleading. In fact, [Forbes reported that about 80% of data science is related to data preparation](https://www.forbes.com/sites/gilpress/2016/03/23/data-preparation-most-time-consuming-least-enjoyable-data-science-task-survey-says/). This includes, among other things, data cleaning, re-scaling, and labelling.\n",
    "\n",
    "If you also include things like keeping track of what you did and why, storing and backing up data generated as well as the data used at the beginning, and recording the evaluation results, data exploration, interpretation, I would say that **95%** of machine learning is involved in **data management**. This can, in fact, be said to be a substantial part of achieving transparency, a corner stone of addressing the ethical concerns regarding AI and bias, fairness, data protection, explainability etc.\n",
    "\n",
    "So, let's get some data!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15b4f64",
   "metadata": {},
   "source": [
    "# Task 2: Getting Data\n",
    "\n",
    "Before anything else, you must get the data! There many ways you can get data. In previous labs, you have already seen that scikit-learn's datasets package has some datasets already available to you. You can also get data from a nuumber of places such as OECD, OpenML, Kaggle, individual repositories in GitHub. \n",
    "\n",
    "In reality, data is everywhere. In fact, artists who make use of AI often make their own datasets: for example, check out [Anna Ridler's shell images](https://annaridler.com/the-shell-record-2021), [Caroline Sinders' feminist dataset](https://carolinesinders.com/feminist-data-set/), and [Refik Anadol's coral images](https://refikanadol.com/works-old/artificial-realities-coral/). While these may be owned by the artists, it can inspire new ways of thinking about data.\n",
    "\n",
    "In this task we will look at something a little less artistic! &#x1F609;\n",
    "\n",
    "- **Example Tabular Data**: dataset comprising housing prices in California in the the United States. This dataset is available on the GitHub, courtesy of Aurelien Geron. \n",
    "- **Example Image Data**: MNIST dataset comprising images of handwritten digits. Handwritten digit recognition with the MNIST dataset is sometimes called the **\"Hello World!\" of machine learning**. \n",
    "\n",
    "We will use these datasets to carry out the prediction of "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656eba8c",
   "metadata": {},
   "source": [
    "## Task 2-1: Download the Data: Example Tabular Data\n",
    "\n",
    "The following code defines **function** called `load_housing_data()`. This function retrieves a compressed file avaialable at `https://github.com/ageron/data/raw/main/housing.tgz` and saves it in a folder `datasets` which is in the same folder as this notebook. This will be created if it does not exist. The code will also extract the contents in the folder `datasets`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56ec696a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import tarfile\n",
    "import urllib.request\n",
    "\n",
    "def load_housing_data(): #defines a function that loads the housing data available as .tgz file on a github URL\n",
    "    tarball_path = Path(\"datasets/housing.tgz\") # where you will save your compressed data\n",
    "    if not tarball_path.is_file():\n",
    "        Path(\"datasets\").mkdir(parents=True, exist_ok=True) #create datasets folder if it does not exist\n",
    "        url = \"https://github.com/ageron/data/raw/main/housing.tgz\" # url of where you are getting your data from\n",
    "        urllib.request.urlretrieve(url, tarball_path) # gets the url content and saves it at location specified by tarball_path\n",
    "        with tarfile.open(tarball_path) as housing_tarball: # opens saved compressed file as housing_tarball\n",
    "            housing_tarball.extractall(path=\"datasets\") # extracts the compressed content to datasets folder\n",
    "    return pd.read_csv(Path(\"datasets/housing/housing.csv\")) #uses panadas to read the csv file from the extracted content\n",
    "\n",
    "housing = load_housing_data() #runsthe function defined above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e678b056",
   "metadata": {},
   "source": [
    "### If you've already downloaded and extracted the compressed file - then the following is all you need"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741051dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "housing = pd.read_csv(Path(\"datasets/housing/housing.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc852dd0",
   "metadata": {},
   "source": [
    "## Take a Quick Look: housing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e780f081",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb46ae03",
   "metadata": {},
   "source": [
    "Note: The result above tells you how many attributes (e.g. longitude, latitude) characterise the dataset. How many are there? The data type float64 is a numerical data type. So, the table above also tells you how many attributes are not numerical. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56390ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add housing head to get a better picture of what the dataset looks like (helps myself with getting an overview of the data)\n",
    "# the head() method outputs the first five rows of a dataset\n",
    "housing.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c0ad40",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing[\"ocean_proximity\"].value_counts() # tells you what values the column for `ocean_proximity` can take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc4b5110",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.hist(bins=50, figsize=(12, 8))\n",
    "save_fig(\"attribute_histogram_plots\")  # extra code\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5900bf86",
   "metadata": {},
   "source": [
    "Finally you can run `housing.describe()` to get a summay of the data set `housing`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3bc88c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24805732",
   "metadata": {},
   "source": [
    "At this point you should stop looking at the data until you have set aside test data. This is to prevent inadvertent bias creeping into the machine learning process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae81bc17",
   "metadata": {},
   "source": [
    "## Task 2-2: Download the Data: Example Image Data\n",
    "\n",
    "In contrast to tabular data, image data sets are not always read in using pandas. Technically you can do this (as the line below commented out suggests) but as there are no features human-friendly features (such as, median income etc.) - only pixel information, it does not always help to load it as a pandas dataframe, unless the model requires it to be so. Use the command `type` to see what data type `mnist` is - you can see that it is not a `pandas.core.frame.DataFrame`. Dataframes are not the preferred **data structure** for image data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae8a9ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "\n",
    "mnist = fetch_openml('mnist_784', as_frame=False, parser='auto')\n",
    "\n",
    "#mnist_dataframe = pd.DataFrame(data=mnist.data, columns=mnist.feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2311961e",
   "metadata": {},
   "source": [
    "## Take a Quick Look: MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe3e0e2e",
   "metadata": {},
   "source": [
    "The command `mnist.info()` will not work here, to get information about the dataset content, because it is not a pandas dataframe. However, since your dataset is part of the `sklearn.datasets` universe, similar tools as those used in the previous lab exercises apply: for example, the keyword `DECSR` - as demonstrated in the first code cell below. The `print` command can be used in conjunction to get some useful context of the dataset structue and origin. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f269f8bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mnist.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c410e32",
   "metadata": {},
   "source": [
    "## Task 2-3: Review the data description above with your group.\n",
    "\n",
    "What is the size of each image?\n",
    "\n",
    "Examine how LeCunn, Cortes, and Burges reorganised the NIST data as MNIST. Note that they remixed the data in two ways to create different a training dataset and test dataset. What they did do? Why do you think they did this? Was it justified? \n",
    "\n",
    "Write down the results in a markdown cell below. I have already created a markdown cell below - just double click to edit the content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55896634",
   "metadata": {},
   "source": [
    "***Mark Down cell for critique***\n",
    "\n",
    "1. The size of the images is 20x20 pixels and made to fit a 28x28 field \n",
    "2. The training and test set were organized by writers, meaning that no images by the same writer are to be found in both datasets. They are only either in the training or in the test set, never in both. This was probably done to make sure that the model is actually learning to recognize handwritten digits and does not simply remeber a certain style by training and testing on similar digits. Therefore, the machine trains and learns on certains writers and is tested on others. Because of that, the images in the test set are wholly unknown to the machine when first running the test data, meaning that the dataset can actually be used to test the accuracy of the recognition. This probably increases the accuracy of the model when using it for unknown digits later on, which thereby justifies the method."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d5fba0",
   "metadata": {},
   "source": [
    "### To see a full list of keys other than `DESCR` which is available to this dataset You can use the command `mnist.keys()` to see more keys available - run the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a1d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed70b9bc",
   "metadata": {},
   "source": [
    "## Task 2-4: Identifying the Dimension of Images\n",
    "\n",
    "You may recognise some of the keys listed above for `mnist.keys()`. For example, you should have seen the key `data` and `target` already in earlier labs. The former will return the image pixel data, while the `target` key will return the labels (the categories or classification) assigned to each of these images. \n",
    "\n",
    "- Create a code cell below to use these keys in Python, to use the `shape` command to verify the number of images in the dataset and how many features (e.g. pixels) represents the image. Print out the shape and the target categories. \n",
    "\n",
    "I have created a cell for you below with the data and target assigned to the **variables** `images` and `categories`. Add a line to print out the shape of the images and the list a assigned categories. Single click on the area to start editing. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae51ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cell for python code \n",
    "\n",
    "images = mnist.data\n",
    "categories = mnist.target\n",
    "\n",
    "# insert lines below to print the shape of images and to print the categories.\n",
    "print(images.shape)\n",
    "print(categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2035aeb5",
   "metadata": {},
   "source": [
    "Note:  shape method for dataframes - result: 70.000, 784 meaning 70.000 images in total which fits the description of the training and test set (mentioned 30.000 from SD-3 and 30.000 from SD-1 for training set and 5.000  from SD-3 and 5.000 from SD-1 for test set in description) and 784 referring to the size of the fields (aforementioned 28x28 = 784)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8f61e6",
   "metadata": {},
   "source": [
    "Let's take a look at one of the digits in the dataset - the first item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb52d34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extra code to visualise the image of digits\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## the code below defines a function plot_digit. The initial key work `def` stands for define, followed by function name.\n",
    "## the function take one argument image_data in a parenthesis. This is followed by a colon. \n",
    "## Each line below that will be executed when the function is used. \n",
    "## This cell only defines the function. The next cell uses the function.\n",
    "\n",
    "def plot_digit(image_data): # defines a function so that you need not type all the lines below everytime you view an image\n",
    "    image = image_data.reshape(28, 28) #reshapes the data into a 28 x 28 image - before it was a string of 784 numbers\n",
    "    plt.imshow(image, cmap=\"binary\") # show the image in black and white - binary.\n",
    "    plt.axis(\"off\") # ensures no x and y axes are displayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f02abd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualise a selected digit with the following code\n",
    "\n",
    "some_digit = mnist.data[0]\n",
    "plot_digit(some_digit)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e690ce",
   "metadata": {},
   "source": [
    "# Task 3: Setting Aside the Test Data\n",
    "\n",
    "To set aside test data, you need to take shuffled and stratified samples. \n",
    "\n",
    "## Why Do We Shuffle\n",
    "\n",
    "The dataset you are working with could be ordered in a specific way (for example, all the data points in a specific class all at the top). If you select a percentage of 20% from the top, you could get data points in only specific classes. By shuffling we can avoid this. As it happens `sklearn` has a nifty function to allow you to split the data inclusive of splitting. This function is called `train_test_split`. See it in action below, using the housing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0ba19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tratio = 0.2 #to get 20% for testing and 80% for training\n",
    "\n",
    "train_set, test_set = train_test_split(housing, test_size=tratio, random_state=42) \n",
    "## assigning a number to random_state means that everytime you run this you get the same split, unless you change the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6040547c",
   "metadata": {},
   "source": [
    "## Why Do We Stratify\n",
    "\n",
    "If the dataset is skewed so that it contains more samples of a specific kind more than others, sampling randomly will result in your test data not representing the population you would like to test. An example of the estimated probability of getting a bad sample that does not reflect the actual population is provided below. The US population ratio of females in the census is 51.1%. The following is the probability of getting a sample with less than 48.5% or greater than 53.5% females if you take a random sample withoput stratifying: approximately **10.71%** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e27820",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – shows another way to estimate the probability of bad sample\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sample_size = 1000\n",
    "ratio_female = 0.511\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "samples = (np.random.rand(100_000, sample_size) < ratio_female).sum(axis=1)\n",
    "((samples < 485) | (samples > 535)).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b4372c",
   "metadata": {},
   "source": [
    "## Task 3.1: Stratified Sample: Housing Data\n",
    "\n",
    "The following code adds a column to the `housing` data to create bins of data according to interval brackets of median income of districts. This is a first step to creating a stratified sample across different income brackets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b0cd11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "housing[\"income_cat\"] = pd.cut(housing[\"median_income\"],\n",
    "                               bins=[0., 1.5, 3.0, 4.5, 6., np.inf],\n",
    "                               labels=[1, 2, 3, 4, 5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ed7fbfe",
   "metadata": {},
   "source": [
    "The following code uses the above bins to implement startified sampling - that is, it will randomly sample 20% (because we set test ratio `tratio` to 0.2) from each income bracket defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69cc591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "tratio = 0.2 #to get 20% for testing and 80% for training\n",
    "\n",
    "strat_train_set, strat_test_set = train_test_split(housing, test_size=tratio, stratify=housing[\"income_cat\"], random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19d7fcdc",
   "metadata": {},
   "source": [
    "The code below prints out the proportion of each income category in the stratified test set above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57103983",
   "metadata": {},
   "outputs": [],
   "source": [
    "strat_test_set[\"income_cat\"].value_counts() / len(strat_test_set) #Prints out in order of the highest proportion first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b1d994",
   "metadata": {},
   "source": [
    "Note the attribute `random_state`. Setting this to a specific number like 42 **keeps the split the same everytime you run the code**. Keep in mind that it will not stay the same if you change the underlying dataset (e.g. adding more). \n",
    "\n",
    "Discuss with your peer group, why a stratified sample based on median income is reasonable. Create a markdown cell below to report on the results of the discussion. I have already create one below, so just double click to edit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6718cca3",
   "metadata": {},
   "source": [
    "***Markdown cell***\n",
    "A stratified sample is reasonable because it helps to prevent certain bias in the conclusions that are being drawn from the use of the dataset. Using the median means that it prevents an odd number from having too much influence. For example in a row of numbers: 1 1 1 1 1 2 9 the median is 1, which is a closer representation of the actual numbers than the averaqge, for which the result would be around 2.3, which is much higher than most of the numbers of the row. In the example of income in housing districts, the median is useful so one person earning for example much more money than the rest does not impact the numbers that much, since that could have a severe impact on the community, when it comes to housing prices or funding. To stratifie the sample is reasonable for a similar reason, so that one outlier does not imapct the result too much and the data is a more accurate representation of the actual state of things.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a6a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(mnist.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15a818f2",
   "metadata": {},
   "source": [
    "Note: if taking a break between Task 2 and 3 and closing the notebook, the cells in which the datasets housing and mnist are being imported have to be executed again to prevent an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270dbdb6",
   "metadata": {},
   "source": [
    "## Task 3.2: Setting Aside Test Set for Image Data \n",
    "\n",
    "In the case of `mnist` the data is already cleaned prepared, scaled and ordered, so that the training data is the first 60,000 images, followed by the test data which is the last 10,000 images. So you need not shuffle and stratify nor use `train_test_split`. Instead, you can use the following code to set aside your test dataset. The data type of `mnist.data` is `numpy.ndarray` (you can verify this with the command `type`). \n",
    "- By using a colon and then 60000 in a square bracket after `mnist.data`, you are telling the computer that you want all the items up until the 60000th item (not including the 60000th) in the array `mnist.data`. We assign this to the **variable** `X_train`. \n",
    "- Likewise, the first 60000 categories corresponding the the first 60000 images are assigned to the **variable** `y_train`. \n",
    "- By using a colon after 60000, you are telling the computer you would like all the items from the 60000th onwards.\n",
    "\n",
    "It is machine learning convention to use upper case `X` for variable names associated with data and lower case `y` in the variable names associated to labels (or categories/classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab94357",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = mnist.data[:60000]\n",
    "y_train = mnist.target[:60000]\n",
    "\n",
    "X_test = mnist.data[60000:]\n",
    "y_test = mnist.target[60000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc35af8",
   "metadata": {},
   "source": [
    "# Task 4: Selecting and Training a Model\n",
    "\n",
    "You are finally ready to select and train your model. In the following code, we will use linear **regression for the prediction of district housing prices**, and a **convolutional neural network** for classification of hand written digits. For linear regression, we will use `Scikit-Learn`. For the convolutional neural networks we will use the `tensorflow` library with `keras`. Regardless of the model, the general flow is similar:  \n",
    "\n",
    "- Import the model from the relevant library. \n",
    "- Create an untrained model instance. \n",
    "- Fit the model to your training data.\n",
    "\n",
    "## Task 4-1: Housing Data and Linear Regression\n",
    "\n",
    "With linear regression, you need data, whose values are continuous - not discrete values such as categories. Note that it is not enough for the values to be numbers, which can also be categories (for example, your place in a queue is a number but is never a fraction like 1.33). The feature `income_cat` is another category expressed as a number. \n",
    "\n",
    "Before doing anything else, let's assign a copy of the stratified training set we created earlier to the variable `housing`. You should always work with copies of data and never look at the test set in case we inadvertently use information in the test to improve the performance (**data snooping bias**).\n",
    "\n",
    "To do this use the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6cbdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f59211",
   "metadata": {},
   "source": [
    "### Step 1: Checking Correlations: Training Set\n",
    "\n",
    "Linear regression in essence works by picking up on correlations between features. So it can be useful to explore the correlations especially between the target value you are trying to predict `median_house_value` and the other features in the dataset, e.g. `median_income`.\n",
    "\n",
    "The training set we have is of type `pandas.DataFrame`.  For pandas, dataframes have the function `corr` which calculates the correlations for you. The code is below - first it calculates all the correlations between all the pairs of features and saves it in variable `corr_matrix`. \n",
    "\n",
    "We can take a look at correlations for `median_house_value` by using that feature name in a square bracket (**with quotation marks!**). the last part `sort_values(ascending=False)` sorts the correlation to display it in descending order of correlation (that is, most correlated fetures are listed first)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4767a968",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = housing.corr(numeric_only=True) # argument is so that it only calculates for numeric value features\n",
    "corr_matrix[\"median_house_value\"].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e741e0d8",
   "metadata": {},
   "source": [
    "### Step 2: Visualise the Correlations\n",
    "\n",
    "Pandas also can visualise these correlations as a graph for you. In the code below, we have selected four features (see the variable with that name), to 4 x 4 grid of graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b7facae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "features = [\"median_house_value\", \"median_income\", \"total_rooms\",\n",
    "              \"housing_median_age\"]\n",
    "scatter_matrix(housing[features], figsize=(12, 8))\n",
    "#save_fig(\"scatter_matrix_plot\")  \n",
    "\n",
    "#The line above is extra code you can uncomment (remove the hash at the begining) to save the image.\n",
    "#But, to use this, make sure you ran the code at the beginning of this notebook defining the save_fig function\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c4e919",
   "metadata": {},
   "source": [
    "### Step 3: Separate the Target Labels from Your Data\n",
    "\n",
    "In any machine learning task, you need to provide the data and the target Label separately to the machine learning algorithm. Otherwise, they have no way of knowing which of the features is the target label. In our scenario, the label for the housing data is the `median_house_value`. When your data is in a padas dataframe format, you can simply 1) drop the column with the label to get the data, and, 2) get the column for the target label, to get the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610bec83",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = strat_train_set.drop(\"median_house_value\", axis=1) ## 1)\n",
    "housing_labels = strat_train_set[\"median_house_value\"].copy() ## 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a9e4b6",
   "metadata": {},
   "source": [
    "### Step 4: Look for Missing Values in the Data\n",
    "\n",
    "When working with tabular data, it is quite common to find that some rows are missing values for some of the columns. If you run the `info` command for dataframes (we've done this in [Task 2-1](#Task-2-1:-Download-the-Data:-Example-Tabular-Data) above!). \n",
    "\n",
    "- Running the code will show the total number of entries. By comparing that number to the number of Non-Null entries for each feature (e.g. `total_bedrooms`) you can see whether there are missing values. \n",
    "- If there are no missing values, these numbers should be the same!\n",
    "\n",
    "How many values are missing for the number of `total_bedrooms'? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bffc540",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31181096",
   "metadata": {},
   "source": [
    "### Step 5: Handling Missing Values\n",
    "\n",
    "To handle the missing values, you need a code in place to tell the machine what to do if there are missing values. In-depth discussion of handling missing values is beyond the scope of this course, but there are three common ways of handing these:\n",
    "\n",
    "- (Option 1) Drop the row with missing value. This causes you to lose data points. In our scenario with the housing data, 168 rows will be removed.\n",
    "- (Option 2) Drop the column with missing values. This causes you to lose one of your features.\n",
    "- (Option 3) Fill in the missing value with some value such as the median or mean or fixed value that makes sense. This is called **imputing**.\n",
    "\n",
    "Depending on which approach you take, the performance of your AI could be different. Also, note that, **with Option 1, you will have to remove the corresponding rows in `housing_labels` before using these in a machine learning task**. Following are codes from each of these approaches. Read the comments included in the code for understanding what each cell is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ee6f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the code for Option 1 above. \n",
    "housing_option1 = housing.copy() #This makes a copy of the data to variable housing_option1, so that we don't mess up the original data.\n",
    "\n",
    "housing_option1.dropna(subset=[\"total_bedrooms\"], inplace=True)  # option 1 - dropping the rows where total_bedroom is missing values.\n",
    "\n",
    "housing_option1.info() #look for missing values after rows have been dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4e11d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_option2 = housing.copy() #This makes a copy of the data to variable housing_option1, so that we don't mess up the original data.\n",
    "\n",
    "housing_option2.drop(\"total_bedrooms\", axis=1, inplace=True)  # option 2 - dropping the column associated with total_bedrooms\n",
    "\n",
    "housing_option2.info() # checking for missing values in the new data after column has been dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dba58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_option3 = housing.copy() #This makes a copy of the data to variable housing_option1, so that we don't mess up the original data.\n",
    "\n",
    "median = housing[\"total_bedrooms\"].median() # calculating mean of the value for total_bedrooms to use in filling missing values\n",
    "housing_option3[\"total_bedrooms\"].fillna(median, inplace=True)  # option 3 - filling missing values with the median\n",
    "\n",
    "housing_option3.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1bf067",
   "metadata": {},
   "source": [
    "Note: for this specific exercise, I would use the first option, because it feels the easiest and most intuitive to me. Furthermore, the dataset has 16512 entries before the removal, meaning that the removal of 168 rows will probably not have a big impact. This decision can be different from case to case. In a dataset with more missing values, for example, the third option might be a better choice than the first one, so as not to loose too much data. \n",
    "\n",
    "To be able to understand better, what each option does, all three cells have been executed, since the result is always assigned to a different variable, meaning that executing all three options does not have an impact on the dataset that is going to be used later on. To understand all three options, it is interesting to compare the values in each of the three outputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf9695a",
   "metadata": {},
   "source": [
    "#### You can also use `SimpleImputer` from the `sklearn.impute` library to fill missing values with the median"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b28f434",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "imputer = SimpleImputer(strategy=\"median\") # initialises the imputer\n",
    "\n",
    "housing_num = housing.select_dtypes(include=[np.number]) ## includes only numeric features in the data\n",
    "\n",
    "imputer.fit(housing_num) #calculates the median for each numeric feature so that the imputer can use them\n",
    "\n",
    "housing_num[:] = imputer.transform(housing_num) # the imputer uses the median to fill the missing values and saves the result in variable X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c1a0b1",
   "metadata": {},
   "source": [
    "### Step 6: Scaling Your Features\n",
    "\n",
    "Machine learning algorithms learn better when similar scales are used across all the features. For example, the numeric range of values for `total_rooms` will be totally different from `median_income`.\n",
    "\n",
    "Test this with he **min** and **max** values after running the pandas `describe()` function. Code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead8cda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb924d5b",
   "metadata": {},
   "source": [
    "You can see that all the features have very different ranges. Bringing these in alignment is called **feature scaling**. There are a number of ways to scale features. Scikit-Learn provides something called MinMaxScaler which scales the values to fit into a range defined by you. Below, the code is provided for when you are fitting it into the range from -1 to 1. AI algorithms often like the mean to be placed at zero, so best to set a range with zero as the mid point value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "396bcce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler # get the MinMaxScaler\n",
    "\n",
    "min_max_scaler = MinMaxScaler(feature_range=(-1, 1)) # setup an instance of a scaler\n",
    "housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)# use the scaler to transform the data housing_num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc23108",
   "metadata": {},
   "source": [
    "Alternatively, Scikit-Learn also provides a method called StandardScaler. This method tries normalise the distributional characteristics by considering mean and standard deviation for each feature, and normalising the values to have standard deviation 1. But, even without knowing the mathematical details, we can simply employ the tools provided by `sklearn` - example below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3eb2e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "std_scaler = StandardScaler()\n",
    "housing_num_std_scaled = std_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bd704e",
   "metadata": {},
   "outputs": [],
   "source": [
    "housing_num[:]=std_scaler.fit_transform(housing_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1018f320",
   "metadata": {},
   "source": [
    "### Step 7: Train a Linear Regression Model\n",
    "\n",
    "In the first instance we will use the data resulting from the `SimpleImputer` (with the **median** as a strategy) and use   `StandardScaler` for scaling the features. Before we train the Linear Regression model for predicting median housing prices for districts, we need to also apply the scaling to the target labels (in our case, the known median housing prices). The code is provided below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483b52a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler #This line is not necessary if you ran this prior to running this cell. \n",
    "#We are however including it here for completeness sake.\n",
    "\n",
    "target_scaler = StandardScaler() #instance of Scaler\n",
    "scaled_labels = target_scaler.fit_transform(housing_labels.to_frame()) #calculate the mean and standard deviation and use it to transform the target labels.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed4b20f",
   "metadata": {},
   "source": [
    "### Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdc4e78d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression #get the library from sklearn.linear model\n",
    "\n",
    "model = LinearRegression() #get an instance of the untrained model\n",
    "model.fit(housing_num, scaled_labels)\n",
    "#model.fit(housing[[\"median_income\"]], scaled_labels) #fit it to your data\n",
    "#some_new_data = housing[[\"median_income\"]].iloc[:5]  # pretend this is new data\n",
    "\n",
    "#scaled_predictions = model.predict(some_new_data)\n",
    "#predictions = target_scaler.inverse_transform(scaled_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129a1e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_new_data = housing_num.iloc[:5] #pretend this is new data\n",
    "#some_new_data = housing[[\"median_income\"]].iloc[:5]  # pretend this is new data\n",
    "\n",
    "scaled_predictions = model.predict(some_new_data)\n",
    "predictions = target_scaler.inverse_transform(scaled_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd4851b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(predictions, housing_labels.iloc[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3980d05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# extra code – computes the error ratios discussed in the book\n",
    "error_ratios = housing_predictions[:5].round(-2) / housing_labels.iloc[:5].values - 1\n",
    "print(\", \".join([f\"{100 * ratio:.1f}%\" for ratio in error_ratios]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcaa0e9",
   "metadata": {},
   "source": [
    "Note: The above cell is not working due to the variable housing_predictions not being defined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f01ebd5a",
   "metadata": {},
   "source": [
    "### Step 8: Cross Validation\n",
    "\n",
    "As mentioned in Lecture 4 - pre-recorded lecture - having one training set and one test set to check performance is limited in producing a robust AI model. What you really want to see is a stable performance across many training sets and test sets. IN the first stance you want to test the model on the training set.\n",
    "\n",
    "One way to evaluate your model before testing on the new data (the data you set aside as your test data) is cross validation. This where you split your training data into many pieces, then leave on of the pieces out for testing.The code below does that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf15d382",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rmses = -cross_val_score(model, housing_num, scaled_labels,\n",
    "                              scoring=\"neg_root_mean_squared_error\", cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ea58a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(rmses).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c936dc9a",
   "metadata": {},
   "source": [
    "## Task 4-2: Hand Written Digit Classification\n",
    "\n",
    "As mentioned earlier, as an example, for the hand written digits, we will use a specific kind of neural network model called a **Convolutional Neural Network (CNN)** model. In the lectures, we learned about general neural networks but not CNN. If you want to get a feel for CNNs, you can watch the Stat Quest Video [Neural Networks Part 8: Image Classification with Convolutional Neural Networks (CNNs)](https://www.youtube.com/watch?v=HGwBXDKFk9I). If you feel confident to go deeper, Chapters 19 and 22 of Russell and Norvig's Book [\"Artificial Intelligence: a modern approach\"](https://eleanor.lib.gla.ac.uk/record=b3897063) is an excellent read, not to mention Géron's book [\"Hands-On Machine Learning with Scikit-Learn, Keras & Tensorflow\"](https://eleanor.lib.gla.ac.uk/record=b4094676).\n",
    "\n",
    "For this task, **we will move away from `sklearn` and use `tensorflow` and `keras` instead**. Tensorflow and Keras are popular libraries recognised for their usefulness in building neural networks quickly. Although we already loaded the data from `sklearn`, in the code below, we will get it again from `tensorflow.keras.datasets`. This will allow you to experience getting data from another library and also make it easier to work with the subsequent code because everything will happen with tensorflow. \n",
    "\n",
    "The data is already fairly organised, so, the data cleaning part of the operation can be abbreviated. This is however not a characteristic of image data. It is a characteristic of **curated data** which is not the same as real world messy data 9such as the housing data from earlier). \n",
    "\n",
    "The code for importing the libraries and getting the data has been included below. To get these to work, **you will need to have your environment installed with `tensorflow` and `keras`**. In the first line of the first code cell below, you will notice that `tensorflow` is imported as `tf`. This is a recognised convention in the machine learning community. Adopting this convention makes your code more readable for this community. Once you have imported the library that way, `tf` will be used subsequently instead of `tensorflow`.\n",
    "\n",
    "### Step 1: Get the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "550ed1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow # only needs to be executed once, if not already installed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31a38b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "542b6908",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "mnist = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b89621d",
   "metadata": {},
   "source": [
    "### Step 2: Review What the Data Looks Like  \n",
    "\n",
    "You can review information about what this dataset looks like at the Keras page for the [MNIST digits classification dataset](https://keras.io/api/datasets/mnist/). The page makes it clear that `mnist` above is organised as a data type called **tuple** - something that looks like `(a,b)`. The `a` and `b` are tuples themselves, representing training and test data, respectively. Check first that mnist is a **tuple** with following line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45cdcbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(mnist))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adf9270",
   "metadata": {},
   "source": [
    "The **Keras webpages are useful** for looking up and getting information about wide range of keras commands you might encouter in machine learning programs. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1805f5f4",
   "metadata": {},
   "source": [
    "### Step 3: How to Get the Data\n",
    "\n",
    "To get the data out of `a` and `b`, run the following code. Read the comment for explanation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c621149f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = mnist \n",
    "# (X_train_full, y_train_full) is the 'tuple' related to `a` and (X_test, y_test) is the 'tuple' related to `b`.\n",
    "# X_train_full is the full training data and y_train_full are the corresponding labels \n",
    "# - labels indicate what digit the image is of, for example 5 if it is an image of a handwritten 5."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a62b91a",
   "metadata": {},
   "source": [
    "### Step 4: Scaling the Pixel Values (the features)\n",
    "\n",
    "In dealing with images, there are four main comsiderations that most frequently arise: \n",
    "- 1) input size of the image (height and width in terms of pixels)\n",
    "- 2) whether you want to move the pixels so that the image is centered in the middle\n",
    "- 3) scaling the value of the pixels to be in a specified range. \n",
    "\n",
    "The neural network we will use will works best with pixel values between 0 and 1. Pixels in a black and white image usually have values between 0 and 255. The code below simply rescales these, dividing by 255. There are other ways of scaling this, similar to when we scaled the feature values of the `housing` data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c876f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full = X_train_full / 255.\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf244fc8",
   "metadata": {},
   "source": [
    "### Step 5: Split the Training Data into Training and Validation Data\n",
    "\n",
    "We already have data split into training and test data. The **validation data is split from the training data** and used to evaluate the performance during training. This is **different from test data** which is completely new data not seen during training or fine tuning. \n",
    "\n",
    "Test data is used for the final test before publishing the results. In fact in many competitions, the test data is **withheld behind an application interface** so that contestants cannot engage in **data snooping**. \n",
    "\n",
    "The code below takes the last 5000 images for validation data. The second line does the same for the corresponding labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9197eb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid = X_train_full[:-5000], X_train_full[-5000:]\n",
    "y_train, y_valid = y_train_full[:-5000], y_train_full[-5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d405ef72",
   "metadata": {},
   "source": [
    "### Step 5: Increasing Dimension to Include Colour Channels\n",
    "\n",
    "An image is usually represented as a (width x height) block of pixels. When presenting your images to the neural network, you need to add an extra dimension to your image representation, to indicate the number of colour channels your images are using. Normally, for a greyscale image this would be 1, while for a RBG colour image it would be 3. \n",
    "\n",
    "All in all you will be submitting something that has shape like `(N, W, H, C)` where `N` is number of images, `W` is the width of any one image, `H` is the height of any one image, and `C` is the number channels (1 for greyscale, 3 for colour). \n",
    "\n",
    "All your images are expected to be the same size as it enters the neural network. \n",
    "\n",
    "The mnist dataset currently has a shape like `(N, W, H)`. Your numpy library allows you to add the required extra dimension. The code below does this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb5d4722",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # you won't need to run this line if you ran it before in this notebook. But for completeness.\n",
    "\n",
    "X_train = X_train[..., np.newaxis] #adds a dimension to the image training set - the three dots means keeping everything else the same.\n",
    "X_valid = X_valid[..., np.newaxis]\n",
    "X_test = X_test[..., np.newaxis]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9f3aba",
   "metadata": {},
   "source": [
    "### Step 6: Build the Neural Network and Fit it to the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a69e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Unlike scikit-learn, with tensorflow and keras, the model is built by defining each layer of the neural network.\n",
    "# Below, everytime tf.keras.layers is called it is building in another layer\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=3, padding=\"same\", activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.MaxPool2D(),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dropout(0.25),\n",
    "    tf.keras.layers.Dense(128, activation=\"relu\", kernel_initializer=\"he_normal\"),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"nadam\", \n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef39d2d4",
   "metadata": {},
   "source": [
    "Note: the training takes some time, but not too much (around 15-20 when I executed it on my laptop), since the code is interating over the whole dataset for 10 times (this is described as epochs) and it can be observed that with each epoch, the accuracy of the model is going up, starting with around 0.941 and going up to around 0.994\n",
    "This can also be seen in the following screenshot of the output of the above code.\n",
    "![image training output, including times and accuracy](screenshots/training_example.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d356c6f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary() # not necessary for the machine learning task."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022105d9",
   "metadata": {},
   "source": [
    "The summary above is not easy to read initially but it is a presentation of each layer. The numbers at the bottom tell you how many parameters need learning in this model. The visualisation can be useful later when you get more used to neural networks if you should continue on to Semester 2."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e414d05",
   "metadata": {},
   "source": [
    "### Step 7: Train and Evaluate the Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a2f27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc4b587",
   "metadata": {},
   "source": [
    "### Comparing with Another Model\n",
    "\n",
    "Below you are provided with code for using something called **Stochastic Gradient Decent Classifier**. This model applies the stochastic gradient descent optimiser (cf. the **nadam** optimiser used with the CNN above) with any number of algorithms but by default it applies it to a **Support Vector Machine**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ed072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the data again from Scikit-Learn, so that we know the image dimens fit for the model!\n",
    "\n",
    "from sklearn.datasets import fetch_openml\n",
    "import pandas as pd\n",
    "\n",
    "mnist = fetch_openml('mnist_784', as_frame=False, parser='auto')\n",
    "\n",
    "# getting the data and the categories for the data\n",
    "images = mnist.data\n",
    "categories = mnist.target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1185aac1",
   "metadata": {},
   "source": [
    "**Normally, we would set aside the test data**. \n",
    "\n",
    "But in this experiement we will abbreviate and use the entire data and evaluate using cross validation, especially since we are not intending, on this occasion, to develop our model with the validation step. **Note that running this might take a while - so be patient!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a621d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "\n",
    "#cross validation on training data for fit accuracy\n",
    "\n",
    "accuracy = cross_val_score(sgd_clf, images, categories, cv=10)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acedfa2",
   "metadata": {},
   "source": [
    "You can see that the accuracies across all the validation runs are far below that of the CNN test results above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e27a131",
   "metadata": {},
   "source": [
    "# Task 5: Reflection\n",
    "\n",
    "That's it! You've reviewed the machine learning workflow. Before you go, let's reflect on a few things together to fill in the gaps!\n",
    "\n",
    "\n",
    "## Task 5-1: Reflecting on the Machine Learning Workflow\n",
    "\n",
    "\n",
    "Get together with your peer group. For the following tasks, you are expected to write a markdown cell describing the workflow required. You are free to include code, but **no Python code is required**. Discuss the following:\n",
    "\n",
    "1. What would you need to do for your code if:\n",
    "\n",
    "- Your were to use your own data (for example, discuss survey data data and photos)?\n",
    "- You were changing\n",
    "    - Your model?\n",
    "    - Your scaling method?\n",
    "    - Your approach to handling missing data?\n",
    "2. What is the significance of cross validation?\n",
    "\n",
    "### Further exploration\n",
    "\n",
    "In this exercise we only considered numerical data from the housing data - that is we left out the feature `ocean_proximity` which is not numerical. Find out about **One Hot Encoding** from Chapter 2 of the [Hands On Machine Learning book](https://eleanor.lib.gla.ac.uk/record=b4094676). Also watch the video on [Word Embedding and Word2Vec](https://www.youtube.com/watch?v=viZrOnJclY0), to get an intuition for **how textual content is transformed into numerical data**.\n",
    "\n",
    "## Task 5-2: Introducing the Tensorflow Playground\n",
    "\n",
    "Before you go, let's play a little bit more with Neural Networks. There is an excellent online resource for this. Go to [playground.tensorflow.org](https://playground.tensorflow.org/). A screenshot is provided below should it help to verify that you have navigated to the right place!\n",
    "\n",
    "(screenshot had to be removed because attachment could not be found and therefore exporting the file to static .html was not possible.)\n",
    "\n",
    "Change your data type to \"spiral\" by clicking on the picture for spiral data on the lefthand side. \n",
    "- The idea is that the point of orange colour is one class and the ones of blue colour is another class. \n",
    "- As the neural network learns you will see the image on the righthand side change background colour (blue/organge) - the class the neural network thinks the points in those regions belong to.  \n",
    "\n",
    "#### Task 5-2-1: Finding small networks that perform well.\n",
    "\n",
    "- Play around with the interface to get a feel for where everything is. For example, add more hidden layers (each layer is represented as nodes laid out vertically) and/or add nodes in any layer. Do this together in your group. \n",
    "- Try to come up with the smallest network that will bring the training loss down to 0.2 or less. The traning loss is indicated on the right hand side - right underneath the label **Output**.\n",
    "- In a Markdown cell below, describe how many layers with how many nodes you had in your network and how many epochs (indicated on the top lefthand corner) for your best model.\n",
    "\n",
    "#### Task 5-2-2: Examine the patterns displayed in the network nodes (see the image above). \n",
    "\n",
    "Discuss in your group and note down in a markdown cell: \n",
    "- what kinds of patterns the neural network might be learning at different layers and nodes. It is difficult to determine this for certain but you can get some intuition by hovering over the nodes in the tensorflow playground.\n",
    "\n",
    "Markdown cells have been included below for addressing the discussions in Task 5. This is for your convenience - modify as you see fit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d01f6a",
   "metadata": {},
   "source": [
    "**Markdown cell for Task 5**\n",
    "\n",
    "### 5-1:\n",
    "1. for using own data, I would need to make sure the data is as unbiased as possible. Then I would have to clean and prepare the data for the model, for example fixing wrong or missing values or ajusting sizes.\n",
    "For changing the model, I would possibly need to change the form of the data I feed into the model. Furthermore, the interpretation of the output would possibly need to be changed as well.\n",
    "When changing the scaling method, the code would need to be adjusted accordingly. \n",
    "When changing the approach to handling missing data, the interpretation of the results need to be adjusted accordingly, since the way of handling of missing data can impact the bias. \n",
    "2. A model should always work well and not only with the specific test and training set. Therefore, cross validation is useful to evaluate the model without having to find completetly new data all the time. By always changing the training set slightly, the Model is being evaluated on varying datasets consisting of smaller data packages - but leaving one of them out - from the original dataset. Therefore cross validation safes time and effort while still testing the model in new ways. This is a resonable step to be taken before testing a model with completely new data. \n",
    "\n",
    "### 5-2:\n",
    "1. 2 hidden layers. 1st with 5 nodes and 2nd also with 5 nodes and it takes around 315 epochs to get the training loss below the value of 0.2. It is also possible with 4 nodes in the second layer, but the trianing loss values are not as stable in that version. Therefore the model with 5 nodes in both hidden layers was chosen as the best model. \n",
    "\n",
    "2. the network goes from completely linear patterns in the first layer, to slightly tilted linear patterns in the first hidden layer to bent patterns in the second layers. At the different nodes the network might be learning the differences between the colors in reference to the x and y axis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a449b9",
   "metadata": {},
   "source": [
    "## Task 5-3 (Optional): Pre-trained Models\n",
    "\n",
    "Before we conclude this notebook, we will momentarily explore the **pre-trained model** VGG-19. This model was trained for computer vision and image classification. It was developed at Oxford but it is often considered to be the next generation model after AlexNet, which won the ImageNet challenge in 2012.\n",
    "\n",
    "The model is introduced here to illustrate an example of a large convolutional neural network, much bigger than that used for MNIST classification task. Note how many more laters are involved, and the total parameters indicated at the bottom is huge. We can talk about this further if you should continue onto the course in Semester 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc6f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.vgg19 import VGG19\n",
    "\n",
    "model = VGG19() ### this will take some time!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543b2700",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2afe775",
   "metadata": {},
   "source": [
    "Final reflection:\n",
    "As shown in this notebook, the machine learning workflow consists of several steps, that can consists of a varying number of substeps. It can also take up a considerable amount of time, which is not always limited to the training of the model. Preparing the data, for example, can be a long process. Taking into consideration missing values and examining which options to use when dealing with them and other things like size of data as well as encoding and tokenizing have to be taken into consideration sometimes.\n",
    "Still, while AI might seem to be a very complicated and difficult thing at first glance, when breaking down the process into its steps, it is not that complicated anymore. And in conclusion it helps to simply get started somewhere, which this notebook is very useful for. It makes for a welcoming and easily accessible entry into the topic of machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c37300",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook you learned about the machine learning pipeline. You reviewed the general workflow from class, and reflected on the workflow in the context of two example cases and data (housing data and minist data). You tried out **Linear Regression** and **Convolutional Neural Net Work**. You also briefly looked at something called a **Support Vector Machine** with **Stochastic Gradient Descent** (not covered in the lectures), comparing the performance for handwritten digit recognition. \n",
    "\n",
    "Any one of these algorithms when looked at in detail, can be quite complex in terms of steps, as seen in the lectures and these labs. However, when using convenient libraries such as `sklearn`, many of them can be implemented in just a few lines. Having said that, where much of the complexity comes in is in preparing the data. And the **data needs more preparing when it is just collected from real world scenarios or sources**.\n",
    "\n",
    "**When data is curated** (such as the MNIST data), there is less to clean and prepare. However, if we are to discuss AI and bias, we need to to critically look at decisions made at the data curation stage. Often these decisions are not as transparent as it could be, which compromises our ability to assess the suitability of datasets, algorithms, and interpretation of results.\n",
    "\n",
    "We also played with the Tensorflow Playground to enhance our intuition for neural networks. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bf00c72",
   "metadata": {},
   "source": [
    "![image of blackboard with well done written on it](well_done.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7012c4e",
   "metadata": {},
   "source": [
    "Source: the picture is free to use without attribution by the pixaby content licence "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
